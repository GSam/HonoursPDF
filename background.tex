\chapter{Background}\label{C:back} This chapter covers related background material which describes the body of related work and attempts to supply the relevant detail for those attempting to examine this document. In Section \ref{S:rustback}, specific details about the Rust programming language and associated tools are explored. In Section \ref{S:refactorback}, the concept of refactoring is further elaborated and brief examination of early work in automated refactoring. In Section \ref{S:exback}, a number of existing refactoring tools are identified along with their respective approaches to perfomring refactorings, in particular, tools related to Go and Scala. \section{Rust programming language}\label{S:rustback}To provide the necessary context for understanding the Rust specific terminology and concepts, this section explores some basic concepts within Rust. An important area for this work to highlight is the Rust compiler, known as rustc, which forms most of the enabling infrastructure that allows refactoring to occur.\subsection{Language features}\subsubsection{Traits}A trait is a collection of methods defined for an unknown type which is denoted by Self. Traits define interfaces, but do not follow the standard notion of inheritance \cite{traitexample15}. Traits can be composed of other traits, and any conflicting methods must be resolved if they occur. Traits allow definition of default implementations and most importantly, traits are implemented for data types -- specifically structs and enums. Essentially, where a type implements a trait, you can be expected to call any of the methods present in that trait on any object of that type. \begin{verbatim}trait Moo {    fn moo(&self); }struct Cow;impl Moo for Cow {    fn moo(&self) {        println!("Moo");    }}\end{verbatim}\subsubsection{Ownership}% [Needs an example from the Rust docs... Based on the above example, the variable [x] owns the vector.]In Rust, variable bindings have `ownership' of whatever they are bound to \cite{docowner15}\cite{rustbook15}. When an owning variable moves out of scope, any resources (including those on the heap) can be freed and manual cleanup is unnecessary. By default, Rust has `move semantics' where by assigning the value of one variable to another, the object is moved to the new variable and the old variable cannot be used to reference this object anymore. Alternatively, there is the option of performing cloning (using clone()) and to default to copying behaviour, much like primitives within the Rust language (e.g. u32 or i32 for 32-bit signed/unsigned integers).While straightforward ownership with cloning probably provides a good deal of the necessary functionality for general purpose programming, it would likely be tedious \cite{docowner15}. For instance, passing in a variable to function would simply `eat' the variable and it would have to be part of the return so that the original variable can regain ownership (fortunately, Rust supports a return tuple). Rust supports references and borrowing, where a object can be borrowed instead of being owned. Borrows must conform to certain constraints: either there are only read-only borrows, or there is a single mutable borrow \cite{docborrow15}. Borrows must also be nested correctly so that a borrow does not exceed the lifetime of (the borrow of its) owner. By adding these restrictions, memory can usually only be modified by one place and it allows compile-time abstractions (without runtime penalty) to ensure memory safety. It does potentially add complexity during coding; when returning references for instance, the compiler might need additional help to infer the borrowing lifetimes of the different parameters and returns. In order to do so, much like typical generics which parameterize functions or types over some generic type, there is the additional of lifetime parameters (which precede any normal generic types and annotated using a prefixing apostrophe) to describe the lifetime of the input and output variables.\subsubsection{Modules}Modules provide a logical unit of code organization that can be used to hierarchically to divide code while managing visibility \cite{docmod15}. The appearance of the `use' declaration allows binding of a module path to a shorter name and to utilize code stored within such a module namespace. Modules can be nested within the same file, created as a file with the name of the module or as a subdirectory with the name of the module.% Macro system?% Redeclaring let bindings?% Error handling (Option<>, Result<>, try!, panic())\subsection{The Rust compiler}As one of the most prominent examples of Rust code, the self-hosted Rust compiler, rustc, stands on the order of hundreds of thousands of lines of code \cite{openhub15}. Due to the fact that it is written in Rust, the build process of the rustc occurs in multiple stages (with prior stages of the compiler compiling newer stages) and requires the download of a compatible snapshot of the Rust compiler (for stage 0) in order to compile correctly \cite{makefile15}. In order to generate machine code, Rust relies on an LLVM backend and their associated LLVM IR (or intermediate representation).In Rust, a `crate' forms a compilation unit \cite{examplecrates15}. In order to compile a crate, a crate root Rust file is supplied to rustc, which merges the contents of referenced modules before the actual compiler is run over it. As a consequence, modules are not compiled individually, only crates. In order to link an external crate, the `extern crate' declaration must be used. This will both link the library and import all items under a module named the same as the library. \subsubsection{Compilation stages}In the Rust compiler, the compilation of a Rust program is divided up into a number of stages \cite{driver15}. \begin{enumerate}\item Parse input -- From the crate entry point, begin building the AST and parsing all the associated files.\item Configure and expand -- Early phases of the compiler, including loading plugins and dependencies, and macro expansion.\item Analysis -- Performs resolution, type checking and other analysis checks on the crate.\item LLVM Translation -- Translate the analysis and AST to an input form for LLVM.\item Run LLVM -- Run LLVM to produce a bitcode, assembly or object file as a result.\item Link LLVM output -- Run the linker to produce a finished executable or library.\end{enumerate}\subsubsection{Save-analysis}A major feature provided by the compiler to enable refactoring, is the ability to produce a summarized analysis after Stage 3 of compilation. This output in the form of a comma separated (csv) file and collates a number of useful artifacts for annotating source code and performing code analysis. In order for rustc to generate this output, a flag must be supplied to the command line `-Zsave-analysis'. Besides the refactoring tool produced in with this work, the save-analysis output is also used by a Rust plugin for DXR, which is a source code browser developed and used frequently by Mozilla \cite{dxr15}.  % https://github.com/nrc/dxr/blob/rust5/dxr/plugins/rust/__init__.py% DXR_RUST_TEMP_FOLDER% Output dir can also specify csv folder, ./XXX/dxr or ./dxr-temp\subsubsection{Analysis of the csv output}In order to provide context to the nature of the output, a brief summary is presented with variables in Figure \ref{Fig:csv}.\begin{fig}\vspace{5mm}\noindentVariable declaration:\begin{verbatim}variable,file_name,"basic_rename.rs",file_line,3,file_col,8,extent_start,38,extent_start_bytes,38,file_line_end,3,file_col_end,9,extent_end,39,extent_end_bytes,39,id,"13",name,"y",qualname,"y$13",value,"y = 20",type,"i32",scopeid,"0"\end{verbatim}\noindentVariable reference:\begin{verbatim}var_ref,file_name,"basic_rename.rs",file_line,10,file_col,20,extent_start,178,extent_start_bytes,178,file_line_end,10,file_col_end,21,extent_end,179,extent_end_bytes,179,refid,"13",refidcrate,"0",qualname,"",scopeid,"4"\end{verbatim}\caption{Example csv output from -Zsave-analysis}\label{Fig:csv}\end{fig}At the top of the csv there is metadata associated with the crate for which the analysis was generated. Following crate definitions, there are lines of the output which correspond individually to declarations and references for variables, types and functions. The first value of each of these lines is the type of information described by the line. In the cases described by Figure \ref{Fig:csv}, they are variable and var\_ref for variable declaration and variable reference respectively. The remaining attributes are mostly shared between the different types of declarations and references. File name indicates the file in which an item is declared or referenced. The next attributes (file\_line, file\_col, extent\_X) concern where in the source file the code in question appears in terms of file lines, file columns, byte indices relative to the file and byte indices relative to the entire crate. In variable, there is the id attribute which corresponds to the node id of the corresponding node in the abstract syntax tree built by the compiler. In variable reference, this attribute is replaced by refid which corresponds to the node id which the reference is referencing. Here they both refer to "13", which indicates they concern the same variable. In static situations, like variable usages, the refid corresponds directly to the associated node for the declaration. However, this is not always the case with method calls and dynamic dispatch. In these cases, it is necessary to refer to an additional attribute called declid. With types, the node id may be aliased with the node id of the constructor instead of the node id of the declaration of the type, and this must be resolved within the refactoring program.\subsubsection{Name resolution}In Rust, a path (within the compiler) corresponds to the concept of a name formed by a sequence of individual identifiers and other supporting information, e.g. std::cmp::PartialEq \cite{docpath15}. Each of these segments might have type or lifetime information, but more importantly, they have a name and a syntax context. The names are stored, or interned, in an interner table and the syntax context provides information to specify where this name might be located. The appearance of a syntax context is due to Rust macros, which allow usage of variables and variable names introduced in a macro without conflicting or aliasing against any names once the macro is expanded or inlined. This behaviour is only part of the functionality provided by Rust macros, which allows manipulation of the AST and more powerful and (type) safe than text replacing macros \cite{keep15}.In many ways, the functionality of Rust macros parallels the functionality aiming to be achieved by refactoring, particularly in the case of renaming. However, the goal of each is divergent enough that the two systems are not compatible. One of the goals of hygienic macros which work with the compiler, is not to introduce a name that will conflict with any existing names \cite{keep15}. This is done with the use of a syntax context, but the (textual) name itself does not actually change. Essentially, it can produce an AST which would not compile within the Rust compiler, however, when converted to source text where the differences in sytax contexts are unavailable, the code could compile just fine. Renamings performed on source text need to understand where the textual names conflict, and reusing a macro based system as currently implemented would only provide any significances internally in the compiler. Extensions to the macro system could be possible and parts of the system made to the advantage of refactoring, however, it appears fundamentally they are not compatible enough for one to be used as a replacement for the other.Paths form an important part of name resolution within the compiler. Forming the first part of the analysis phase in the compiler, name resolution walks the entire AST and attempts to check that the different paths exist at the contexts in which they were used or their lexical scope \cite{driver15}. In the case of refactoring, this is an important piece of functionality which needs to be used: First to ensure that a proposed name for a refactoring (or renaming) doesn't already exist and will not conflict, and secondly, that once any references to a name are changed, they do not refer to a different name. This is despite potentially being spelt the same and the consequence of different scoping or shadowing rules.\section{The background of refactoring}\label{S:refactorback}Refactoring is critical part of a programmer's life in ensuring that software remains maintainable and enables future extension. Martin Fowler's definition in 1999 defines refactoring as the following: \emph{``The process of changing a software system in such a way that it does not alter the external behavior of the code yet improves its internal structure.''}When intiating a first prototype, there is often little regard for underlying structure or overall design and in many cases, these prototypes live on beyond their original intention \cite{foote1997big}. In order to fight back against poorly structured and code which has degraded over time, a good strategy is to refactor relentlessly. In many cases, the problem context might have changed over time or a better approach has been identified. Being able to modify the code to restructure it prevents the cost of rewriting from scratch, however care must be taken to ensure that the modifications preserve the original behaviour as much as possible.Refactoring can provide a number of benefits. It allows improving the design of code, and reversal of code decay which occurs by addressing short term goals and failing to adhere to the overall structure. It allows better understandability of code, allowing better obedience of convention and conversions to well known idioms. Refactoring encourages code reuse by encouraging extraction, generalization and identification of duplicate pieces of code. By improving design and testing assumptions, refactoring allows programmers to identify bugs in their programs and in general, improves productivity, allowing developers to work together better.Bill Opdyke in Refactoring Object-Orientated Frameworks defined behaviour preservation in terms of seven properties \cite{opdyke1992refactoring}. Although taken from a C++ perspective, the definition continues to be used more widely \cite{schafer2010specification}.\begin{enumerate}\item Unique superclass -- After refactoring, a class should have at most one direct superclass, which is not one of its subclasses.\item Distinct class names -- After refactoring, each class name should be unique.\item Distinct member names --  After refactoring, all member variables and functions within a class have distinct names.\item Inherited member variables not redefined -- After refactoring, an inherited member variable from a superclass is not redefined in any subclass.\item Compatible signature in member function redefinition -- After refactoring, if a member function in a superclass is redefined in a subclass, the two function signatures must match.\item Type safe assignments -- After refactoring, the type of each expression assigned to a variable must be an instance of the variable's defined type.\item Semantically equivalent references and operations\end{enumerate}The first six properties can be verified by a single run of the compiler correctly succeeding, however the seventh property cannot be ensured with the same action. Essentially, the last property is to ensure that two runs of a program with the same inputs will always produce the exact same outputs as the original, unchanged program. There are a number of very simple cases where compilation may succeed while the functionality of a program has changed, one major cause of which is shadowing. The following is an example in Rust:\begin{verbatim}let a = 2;let b = 4;let c = a + b;\end{verbatim}\begin{verbatim}let b = 2;let b = 4;let c = b + b;\end{verbatim}%\begin{multicols}{2}%\lipsum[1-2]%\end{multicols}%\noindent\begin{minipage}{\textwidth}%\begin{minipage}[t]{0.5\textwidth}%\begin{lstlisting}%let a = 2;%let b = 4;%let c = a + b;%\end{lstlisting}%\end{minipage}%%\begin{minipage}[t]{0.5\textwidth}%\begin{lstlisting}%let b = 2;%let b = 4;%let c = b + b;%\end{lstlisting}%\end{minipage}%%\captionof{figure}{Two side-by-side listings}\label{fig:listings}%\end{minipage}%\begin{multicols}{2}%\lipsum[1-2]%\end{multicols}In the first case c evaluates to 6, but in the second case, c evaluates to 4 due to shadowing. This is slightly unique here with local variables since Rust allows duplicated variable bindings under the same name, unlike other languages, for instance Java. Such cases are particularly worrying from a programmer's perspective since compilation failures clearly indicate issues but here successful compilation would provide no indication of preserved behaviour. The program would simply function differently without the programmer necessarily noticing any changes. Although this might be caught with testing, having such a comprehensive test suite for every such eventuality is impractical.%\subsection{The purpose of refactoring}\subsection{Fowler's take on refactoring}ExtractionInliningRenaming[Which refactorings should I describe? Just the ones I reference?]\subsection{Precondition based refactoring}While defining behaviour preservation  in his thesis, Opdyke also details a precondition based approach to performing refactorings \cite{opdyke1992refactoring}. In his work, he examines a number of refactorings individually and outlines a number of preconditions which must apply for each to ensure their correctness. Although not fully comprehensive, the descriptions cover enough detail to compare against and the scope of the refactorings covered easily covers the scope of this work.\section{Approaches to refactoring}\label{S:exback}\subsection{The Go programming language}\subsubsection{gofmt tool}The gofmt tool was originally designed to pretty-print the AST for Go source code. This allowed developers to much more easily follow coding convention by using an automated tool to adjust their code. The gofmt tool was extended to allow a limited, but flexible refactoring capability \cite{gofmt15}. The tool functions by parsing two Go compatible expressions as the input expression and the output expression. Any expressions which matched the input expression underwent the transformation into the corresponding output expression. This was more powerful than a simple text-replace because the expressions has to match the corresponding AST nodes and there is no need to account for other irrelevant details such as whitespace. Whitespace in particular can be solved with a text replace, but requires more complicated regular expressions to match all the possible occurences. This functionality of gofmt allowed the Go team to build the gofix tool which allowed Go user-code to be fixed during the earlier stages of the language when the API had not been set entirely in stone yet \cite{gofix11}.\begin{quote}``Gofix has already made itself indispensable. In particular, the recent reflect changes would have been unpalatable without automated conversion, and the reflect API badly needed to be redone. Gofix gives us the ability to fix mistakes or completely rethink package APIs without worrying about the cost of converting existing code.'' -- Russ Cox, Google Developer \cite{gofix11}\end{quote}\subsubsection{gorename tool}Although gofmt allows some checking of compatible transformations, it cannot always ensure that the resulting transformations would still compile or that new identifiers would not cause conflicts. Recently in 2014, the gorename tool has been created which allows the type-safe renaming of most Go identifiers, whether they are type names or variables for instance \cite{gorename15}. One of the key observations that can made from inspecting the code is that a form of name resolution is used to identify potential renaming conflicts. In this manner, it does not appear to be the case that full compilation is necessary to check the validity of renamings.Comparing again to the gofmt tool, gofmt has the advantage of being more flexible by allowing functionality beyond renaming. By being able to manipulate the AST, gofmt helps to better facilitate a number of the various refactorings highlighted by Fowler.\subsection{The Scala refactoring tool}Built as part of the master's thesis for Mirko Stocker \cite{stocker2010scala}, the Scala refactoring tool forms one of the major refactoring tools for the Scala programmiing language. According to the homepage scala-refactoring.org, the currently suppported refactorings are renaming, extract local, inline local, extract method, organize imports and tentatively move class \cite{scala15}. The major focus of the master's thesis was the extract method refactoring, a non-trivial refactoring which allowed selection of a number of expressions to be replaced with a new method with the replaced expressions appearing in the definition.\subsubsection{Overview of the approach}In this tool, the refactorings are built around a core library which is built around manipulating the AST. Analyzing the AST, the tool builds a number of indexes, including a global index in order to identify corresponding occurences of symbols in the AST. In this way the tool can identify both declarations and usages of different items, and do so as efficiency as possible using indexing. By manipulating the AST, the original source code can be manipulated arbitrarily and accurately in a way which will always continue to compile correctly. But notably, this relies on the ability for pretty-printing based on an AST.\subsubsection{Limitations}One of the major limitations of working solely with the AST is that the correspondence to the source code is somewhat lost in the process. Although pretty printing can occur, syntactic sugar within the language means that the resulting code may be significantly different to the original code and modify much more than the refactoring desired. Furthermore, certain constructs defined entirely in syntactic sugar cannot undergo refactoring due to their absence in the AST. The author does actively attempt to remedy this problem by applying an algorithm to line up the output code to the original source based on certain landmarks present in the text. However, this is not a trivial operation and serves as significant complexity being an operation which may not always succeed.\subsection{Functional programming refactoring tools}The Haskell refactoring tool (HaRe) project is an example of a refactoring tool build for a functional programming environment. Described in the PhD thesis of Brown \cite{brown2008tool}, it was found that Haskell mostly helped facilitate simple, atomic refactorings, and based on a case study, composition of these atomic refactorings was more useful than standalone complex refactoring. Haskell in particular was found to be quite difficult to refactor with many complexities in the language: type signatures, pattern matching, guards, where clauses and type classes. In many cases, complex refactorings posed intriguing questions as to the intended behaviour of a refactoring and imposed questionable default behavour. In many cases, only a tractable subset of possible cases were treated and only a partial solution supplied. Brown found that there were three main refactorings which were commonly used in Haskell: \begin{enumerate}\item Generalize definition -- Selects an expression to generalize to a function parameter\item Introduce new definition -- Select code to be expressed as a new function\item Folding -- Replacing all expressions which can be expressed with a given function\end{enumerate}Folding in particular requires duplicate code detection, and existing work has been with various approaches, at the text level, at the token level and at the AST level. HaRe deals with duplicated AST which is generally more costly than the other approaches. However with AST representation, individual names may be discarded and more sophisticated detection of clones with semantic analysis may be applied. \subsection{Java refactoring}\subsubsection{Overview}In Java, refactoring tools generally rely upon the use of compilation units which are ordinarily defined as source files (with the Scala refactoring tool functioning much in the same manner), allowing incremental compilation of Java programs. Refactoring with Java generally consists of  a number of steps. The first is determining a set of affected compilation units, using a search engine or database. Secondly, the Java Model API allows implementation of preconditions like the presence or existence of some member. Third, manipulation of the abstract syntax tree and then finally code generation \cite{baumer2001integrating}. \subsubsection{Eclipse IDE}The Eclipse open source platform provides an IDE with a number of refactorings available for Java (and a few other programming languages) \cite{widmer07}. Beyond just providing editing and refactoring capability, Eclipse provides the Java Development Tooling platform (JDT) with API for programmers to build their own refactorings. Eclipse supports safe renaming and move refactoring as well as more complex refactoring like extraction or inling of methods or variables. Eclipse supports a number of inheritance based object orientated refactorings like allowing users to replace occurrences of a type with one of its supertypes if possible. Eclipse also allows restructuring at the file structure level, allowing renaming of source code files and classes. Eclipse also provides history functionality for when and where a refactoring has occurred in a project. [list of refactoring?]\subsubsection{JRRT and aspect orientation}Taking an approach different to those previously taken in Java, JastAdd Refactoring Tools (JRRT) attempts to perform refactoring with the use of aspect orientation \cite{schafer2010specification}. Testing against other refactoring tools for Java (although this was in 2010), they found that a number of other refactoring tools, including Eclipse, failed to account for tricky edge cases and precondition based refactoring either imposed too great of a restriction on refactorings or they were too weak to preseve Opdyke's 7th constraint. JRRT incorporates parts of aspect orientated programming in order to achieve a number of refactorings constituting most of the core of the Eclipse tools they built upon. JRRT uses static aspects, instead of dynamic aspects allowing instrumentation of code by crosscutting behaviour within these aspects. Aspects form modular units of behaviour changes and define inter-type declarations which correspond to members of existing types and contexts. After the necessary modifications are made to the AST, the aspects are then converted to source code.In a dissertation documenting JRRT by Max Schäfer, an author of the JRRT, he comments on the difficulty of defining program behaviour and behaviour preservation. The definition should be realistic, not just an idealistic approximation; it should be precise and define exactly what it means to preserve behaviour; and it should be general enough that refactorings typically considered behaviour preserving can also be found under this new definition. Opdyke's definition is appealing due to its generality and application to languages beyond which it was defined, but the definition lacks precision. In Java specifically, changes considered behaviour preserving by Opdyke, like adding an unused field, might modify behaviour in a distributed Java application setting where serialization occurs affecting memory layout and IO behaviour. Schäfer notes the lack of progression on a more concrete definition since Opdyke and takes the more practical approach, much like existing work by Roberts \cite[p. 111]{schafer2010specification}, of defining behaviour preservation in terms of individual refactorings. A large number of work focuses only on test suites passing and refactoring being correct on a subset of programs. Formalization appears to have made little headway in this area, with work only dealing with unrealistic subsets of languages, like Java, which do not address any of the significant complexities that are associated with refactoring. Such limitations appear to extend to the world of functional programming as well, with verification under small subsets of a language and only informal arguments of correctness, which we can note with Brown of the Haskell Refactorer project who also comments on the localized nature of their transformations despite the opposite trend in general refactoring \cite{brown2008tool}.More on IntelliJ IDEA? 