\chapter{Refactoring Rust}\label{C:wd}

[Some description to add?]
\section{The general approach to refactoring}
%\subsection{The general approach to refactoring}
The Scala Refactoring tool describes a general approach to refactoring which can serve as a guideline for what to provide and what goals should be achieved when designing a refactoring tool \cite{stocker2010scala}.
%[Taken from http://scala-refactoring.org/wp-content/uploads/scala-refactoring.pdf to transcribe]
\begin{enumerate}
\item provide a user interface so that a specific refactoring can be discovered and invoked from the IDE.
\item analyze the program under refactoring to find out whether the refactoring is applicable and further to determine the parameters and constraints for the refactoring.
\item transform the program tree from its original form into a new refactored form according to the refactoringâ€™s configuration.
\item turn this new form back into source code, keeping as much of the original formatting in place as possible and to generate code for new parts of the program.
\item present the result of the refactoring to the user, typically in the form of a patch, and apply it to the source code.
\end{enumerate}

%[Figure describe work flow of refactoring or structure of the library]

%Caching to provide multiple refactorings in a single run and to only run the save-analysis where necessary again?

\section{The different conflict types for name introduction}\label{S:different}
Performing renaming without any of the necessary checks is not a particularly difficult task, and one which could be approached with straightforward text-replace. What should be considered when performing an accurate refactoring is the potential to change behaviour and cause conflicts. Fundamentally, there are three different conflict types that occur with lexically scoped items. The examples here are not tailored for any specific language and the naming convention is taken from the comments of the gorename tool \cite{gorename15}.

\subsection{Super-block conflict}
Super-block conflicts occur when a new name coincides with one declared in an outer enclosing block. In this situation, any references to the name in the outer block could be shadowed by the new name.

\begin{fig}[H]
\begin{verbatim}
              int A = 1;                         int A = 1;
              int B = 2;                         int B = 2;
              {                                  {
                  int A = 3;                         int B = 3;
                  print B; // 2                      print B; // 3
              }                                  }
\end{verbatim}
\caption{Super-block conflict: Renaming block local A to shadow outer B}
\label{Fig:super}
\end{fig}

\subsection{Sub-block conflict}
Sub-block conflicts occur when a new name coincides with one declared in an inner sub-block. In this situation, any references to the name in the outer block when changed to the new name might be shadowed by the existing declaration in the sub-block.

\begin{fig}[H]
\begin{verbatim}
              int B = 1;                         int A = 1;
              {                                  {
                  int A = 2;                         int A = 2;
                  print B; // 1                      print A; // 2
              }                                  }
\end{verbatim}
\caption{Sub-block conflict: Renaming outer B forces block local A to shadow outer A}
\label{Fig:sub}
\end{fig}

\subsection{Same-block conflict}
In other languages, this normally occurs with local variables which appear in the same scope. However, as described earlier, let bindings allow the redeclaration of variables under the same name in the same scope. In Rust, this allows mutability to be modified while retaining the original name and is generally considered good practice. While this conflict doesn't occur in Rust in the context of local variables, they still occur with global static variables, fields, and works similarly with other constructs like methods and types.

\begin{fig}[H]
\begin{verbatim}
              int A = 1;                         int A = 1;
              int B = 2;                         int A = 2;
\end{verbatim}
\caption{Same-block conflict: Renaming B to conflict with A in the same scope}
\label{Fig:same}
\end{fig}


\section{Building renamings}\label{S:br}
When performing a renaming, there are two main operations that need to be performed:
\begin{itemize}
\item Finding all accesses of a declaration
\item Finding the declaration of an acesss
\end{itemize}

%\subsection{Access construction}

All of this information can be found in the save-analysis data, however, it is completely static and simplified. In order to be able to perform these operations in the general case, the compiler has to be run again. For a refactoring to succeed, all names in a refactored program must bind to the same declaration as the original program \cite{schafer2010specification}. All original uses should be updated to bind to the renamed declaration and any other usages binding to a different declaration, remain binded to a different declaration.

\section{Building inline-local}\label{S:buildIL}
Of the available literature, it appears that the authors of the JRRT describe the act of inlining a variable in the most specific detail. At the time, they also note the existing scarcity of indepth documentation for specific refactorings[?]. Working with Java in particular, they note that due to the limitations of Java, it is impossible to absolutely ensure 100\% correctness under even common circumstances. In this section, a description as best as possible within the context of Rust will be shown and how despite promising additional guarantees such as mutability, absolute correctness is still quite out of reach.

For this analysis, in order to reduce the problem space (as well as accurately quantifying it), a number of assumptions have been made. First of all, only inlining of standard local variables will be discussed and not variable-like items such as function parameters. There is also the assumption that any code marked as unsafe (which does not follow the usual Rust ownership rules) should not interfere with the refactoring. Furthermore that there only exists completely sensible destructors and operator overloads (in other words, impliict behaviour not necessarily defined at the same location) e.g. ones that do not somehow modify a global variable. Pure functions used to be able to be checked in Rust but due to diffculty in definition, they were dropped. Compare this to Haskell folding. This cannot be solved in Rust, like Java. In Figure \ref{Fig:funcinline}, we can see how the inlining of a database call which might insert a single record will suddenly be repeated if it is inlined. Constructors - singletons...

\begin{fig}[H]
\begin{verbatim}
    let a = insert_into_db();          // After inlining a
    println!("{:?}", a);               println!("{:?}", insert_into_db());
    println!("{:?}", a);               println!("{:?}", insert_into_db());
\end{verbatim}
\caption{Functions violating behaviour preservation with inline local}
\label{Fig:funcinline}
\end{fig}

\subsection{Description}
If there is exactly one usage of a local variable in an inline, then due to uniqueness constraints in Rust, there really is just a single usage without any aliases. This is unlike C++ for instance, where some other pointer could still refer to the same section of memory.

Regardless of mutability this should work. If any variable composing the has been redeclared (new let binding), then the inline should not work. Furthermore, if some part of the underlying expression is mutated, or if there consists of some impure function (or we simply assume conditionally that it is pure -- or effectively pure for the context of the refactoring).  The identification of mutable parts of an expression is also practically impossible given the current system, but work could likely be done to mend this due to borrows (effect system, recursive analysis of origin of memory). 

In the multiple value case this all still applies, you have to make sure that the declared variable is not mutable, or if it is, it doesn't need the mutable. Now there is also the case of refcells and interior mutability. In the multiple case value, any direct aliases have a slightly different effect... copyable vs references. Must always follow the ownership system, so that anything that would've been valid before, has to be valid now e.g. no use of moved objects. 

\section{Building reify and elide lifetimes}\label{S:buildreielide}
Curiously enough, the elision rules as specified in the RFC [source?] specify exactly what steps to take in order to reify, not elide. The rules describe basically how the compiler performs reification of missing lifetime parameters internally. In order to build an elide tool, the steps have to be taken in reverse [include a proof of reversal?]. We can also note that the operations are not inverses of each other i.e. applying an elide after a reification will not necessarily return the original declaration. This is due to the ability to partially specification of lifetimes [example?]. The idea was to built the two operations based on the RFC as close as possible using variations of the RFC examples to test.

%One way of performing renaming is to use access construction. This assumes that we have a procedure to `construct an access', which when given a position and declaration, will bind to that declaration at that position. To rename, you first compute the declarations for every usage. You rename the declaration and then go through every name and construct an access which will bind to the declaration it bound to originally, replacing the original name. Using the process of constructing accesses, new names may have additional qualification, but will still yield a program with the same binding structure. Elide doesn't handle anything non-trivial, lifetime bounds, usage of a lifetime variable twice...

\chapter{Implementation}\label{C:impl}

In this chapter, details of the design are elaborated with greater detail on the technical specifics and Rust specific context. A number of the hurdles and difficulties in implementation arise from shortcomings in the compiler, but also due to specific design decisions in the compiler which led to exposure of Rust specific oddities. The tool itself is known to be compatible with 23.09.2015 Nightly build of the Rust compiler... [Not complete] In Section \ref{S:building}, the facilities for providing verification of renames are explored. In Section \ref{S:changes}, an overview of the changes required, predominantly in the compiler, are described. In Section \ref{S:limit}, a brief overview of some known limitations.

% Redeclarations made under the same scope
% Every approach seems incredibly intrusive.

% With API changing, one of the first to implement anything which interfaces in this particular manner.

\section{Decisions made in the beginning}
\subsection{Choice of language}
The choice of implementing the refactoring tool in Rust is for a number of reasons. Without a strong background in the language, performing even manual refactoring would likely be more difficult. The Rust compiler is a major component of the project and being able to modify the compiler and to understand the function of the compiler has been incredibly important. This is especially the case when documentation is not up to date and the level of support for some feature or API is unknown. It is also critical when there are bugs in the compiler that need to be fixed and extensions are required to facilitate refactoring. The tool itself could also be used to test its own source code. Lastly, a number of API and interoperability problems would likely be avoided and so limiting compiler interactions to Rust code appears to be a favourable idea. [Knowledge of Rust had to be built from the ground up, along with a number of completely unfamiliar language concepts.]

\subsection{Standalone binary vs library}
While the tool is written in Rust, it also functions as a library and should allow for interoperability with any possible tool, GUI or otherwise. The choice of creating a library vs. a standalone tool or binary has created a number of arguments back and forth, but at the moment, the functionality remains provided by a library-type interface. There are reasons that this may not be ideal, for instance, the compiler driver API in Rust for calling the compiler functions primarily on disk. Doing so introduces a number of dependencies for the library and forces specific configurations for setting up the library, ones which would be made simpler with a single tool and no requirement for genericity. Additionally, in order to preserve state over a number of compile runs or to provide better caching, the library may be better off abandoning statelessness and run as a general, potentially background, program with a well defined API instead of a regular library. On the other hand, providing a library allows linking against arbitrary programs, and expectations of a library ensure that the aim of providing key interfaces to refactor code is fulfilled.

% Error propogation system in terms of nesting, expectations on thread serialization
% Throwing exceptions... no null types

\section{Defining the entry point}
\subsection{Identification of affected nodes}
In order to map a variable, function or type to the corresponding AST node, the save-analysis output must be provided. With the csv output, a user need only present a file line and column to determine the node id of the referenced element. Within the library, the csv is read every time this operation is required and will always perform a full scan of the file lines. While this could be avoided, there is still the fundamental issue of providing the save-analysis output as input to the tool, ensuring a full scan will always be necessary regardless. A binary search mechanism for code spans (or sections) could be particularly efficient for searching for a node, however this would still need some mechanism for long running updating of a code map to be effective. Furthermore it is unlikely that this operation presents any significant penalties compared to those encountered with the validity checking of a refactoring to be examined.

\subsection{Simple command line}
With the library, a simple command line tool has been provided to give a user interface for a refactoring to be identified and invoked. The command line tool takes any new names required for a refactoring and takes the original name and code location (typically a declaration) which may be denoted with row and column numbers in the form \textless{}name\textgreater{}:\textless{}row\textgreater{}:\textless{}col\textgreater{}. Row and column may be replaced with -1, as a wildcard, to intiate any refactoring valid for a matching name (where the expectation is that only one such name will be found). The tool also takes the operation that should be undergone (var, type or fn), the save-analysis file, the source file and with that, executes the refactoring, outputing the result to standard output.

\begin{verbatim}
simple.rs file:
fn main() {
    let a = 1;
    let b = a + 1;
}

% refactor var dxr-temp/unknown_crate.csv simple.rs  a:-1:-1 c
a -1 -1
NODE ID: 9
fn main() {
    let c = 1;
    let b = c + 1;
}
\end{verbatim}

The above describes a simple renaming of local variable `a' to `c'. The tool has identified the corresponding node in the AST as having id 9 and has successfully carried out the renaming. If the tool fails, like when it finds a conflict, a simple error message `CONFLICT' is displayed.


\section{Building renames with rustc}\label{S:building}
\subsection{Name resolution for renaming}
Given a node id, a new name, the save-analysis file and the crate root file, a rename refactoring then has enough information to begin. Loading in the csv analysis, there are two separate pieces of information that need to be identified: the declaration and the references. Once they are ascertained, we run the compiler API to invoke the compiler. Using name resolution within the compiler, we can attempt to resolve the new name at the declaration site in the AST to ensure that it does not cause any conflicts. By doing so, this would avoid same-block conflicts and prevent all super-block conflicts. Consequently, this also prevents a number of valid renamings where there is no eventual usage of the shadowed item. This issue is partially addressed in Section \ref{S:limit}. This check does not address the issue of sub-block conflicts, however. In order to do so would require name resolution to resolve the new name at each of the reference sites in the AST to ensure that it does not resolve to an undesired declaration. 

Referring back to the conditions listed in Section \ref{S:br}, resolution at the declaration site for super-block and same-block conflicts forces usages binding to different declarations to remain binded to their different declaration. By addressing sub-block conflicts, at each renamed usage, name resolution would check the remaining condition that the binding was made only to the renamed declaration. In the ideal case, name resolution would run with both the declaration renamed and the usages renamed and within a single pass of the compiler.

Unfortunately, limitations imposed by the structure of name resolution and the internal representation mean that this is not possible. In order to provide functionality for detecting the missing sub-block conflicts, recompilation of the entire crate with a single use renamed is necessary. Of course this provides significant overhead, however, hopefully name resolution can provide the required functionality in the future. Apart from compilation, there does not appear to be any straightforward way to checking if a name already exists in the context for a usage. The full name resolution approach is one which appears to be adopted by gorename \cite{gorename15} and is much more efficient in general due to the fact that only one compiler run should be necessary to check every modification point. The additional choice of employing the full compilation approach for declarations indicates further complexities in providing a valid expression constructions (to test the presence of an existing name). A generic approach could not be used and so constructions of different forms for variables and variations of types and functions would be necessary -- which might not be compatible with simple ad-hoc replacement at the source code level.

\subsection{Compilation run}
Adopting the compilation approach, each reference is renamed to the new name one at a time and compiled to ensure that it fails. If a compilation succeeds, then a super-block or sub-block conflict would have occurred in this location and the refactoring must be halted. Care must be taken to ensure that the compilation fails due to a name resolution problem and not one which is due to other failures. If all the compilations fail correctly, the refactoring proceeds and performs all renamings of the occurrences of a variable/function/type.

\section{The changes to libresolve}\label{S:changes}
In order to provide the necessary capabilities of name resolution, a number of modifications had to be made to the libresolve package within the Rust compiler. Name resolution occurs by walking the AST and resolving as it goes. As it proceeds through the AST, it maintains a list of ribs which correspond to lexical scopes and the various declarations made within them. By doing this, names defined within scopes can be checked, however unfortunately this means that libresolve and the associated resolve\_path call required for resolving a new name in the form of a path is not stateless. The module is built with resolution of an entire crate in mind and so every time a path resolution is required, the entire AST must be walked to find a single node. Compared to compilation, the cost should still not be significant, but there is still the challenge of stopping the walker (as part of the Visitor pattern) in the middle of a traversal. 

\subsection{The lack of inheritance}
Had Rust implemented simple, single inheritance, creation of a walker to terminate at a given node would be quite trivial. An obvious alternative would be to single copy-paste the name resolution walker and modify the functionality as per necessary. Unfortunately, even if the changes were accepted upstream, this demanded heavy modification to a number of interfaces and duplication of further code which relied on the name resolution walker as the only possible type of name resolution walker. Basically it was never built for a generic implementation. Inheritance is a proposed addition to Rust, however, little progress has been made, and there are a number of outstanding issues as to how it would fit in with the existing type system [source?].


The second attempted approach was to attempt delegation to simulate the use of inheritance. Wrapping the outer API of the existing walker with a new walker is quite simple, however, reverting control back to the new walker is not so trivial as calls will normally just continue with the internal walker and not with the wrapped one. In some situations, refactoring of the code to force the walk\_X functions to occur at the end of each visit function would allow very little overall duplication of code, however if multiple walk\_X calls are made within a single function there is no simple solution without modification to the old walker. Even performing this proposed modification causes difficulty due to Rust and the mandatory requirement of ownership. In order to simulate inheritance, one approach would be to have the existing walker hold a field which contained a reference to either itself (for the default behaviour) or the new walker (for the new behaviour). Unfortunately, an object with a reference to itself under normal circumstances is quite difficult and prevented by the compiler due to move semantics. The ideal of one (modifying) reference to any object is broken with any cyclical or circular referencing. There are ways around this, otherwise Rust would suffer in flexibility, but most of them require planning ahead like the use of reference counting. The RC\textless{}X\textgreater{} type is a reference counted pointer and solves any problems with creating circular graphs, however to use them for name resolution likely required conversion of the entire library.

Accepting that reference counting would entail much more work, the last approach was to hand a callback to the resolver to invoke at every AST node. It functions generally for what is required but in terms of modifying without changes to original implementation, inheritance still appears to be the ideal approach (for the Visitor pattern).

\subsection{resolve\_path termination and the issue of panics -- too much detail? to remove?}
Now with a callback, and identification of the correct node location, the question is: What to do now? Deeply nested in the AST tree, the callback cannot simply halt the walker and leave it in the correct state to query the local ribs for lexical scoping. It is possible to simply panic and the stack unwind can be caught, however, the unwind mechanism is not built for general message passing (and nesting these captures is not recommended). In particular, information that can be passed through the panic should be serializable and the implementation of the resolver is not compliant with that, requiring a number of changes.

The resolution itself could be executed in the callback, however, the resolver now owns the callback and therefore makes it impossible to pass the resolver through as an argument of the callback due to the ownership system preventing two simultaneous mutable borrows. Therefore, this is not feasible under the current structure.

The remaining solution is to simply flag the resolver as complete and detecting this flag, perform no additional processing. The no additional processing is absolutely crucial due to the presence of the local ribs which are normally popped off as the scopes are exited. This appeared to be the only remaining practical solution to the issue of stopping the walker.

\section{Building inline}
Only a description has been explored, there is some checking, but the effects system, no pure functions... none of that can actually be verified, so arguably not much better than any other languages that don't bother with mut at all. Interestingly enough, mut needs purity to ensure this type of refactoring... The idea is to first walk the AST for the replacement expr, then look at what variables it uses. Then it tries to replace each in the AST using a folder (trait), but at least point, it goes through all the variables and checks if they resolve to the same declaration they did in the declaration...

\section{Building reify elide}
Reify straightforward, but elide was not so simple. Still lacks all the possible cases... The reintroduction of lifetime parameters was based on the implementation of error reporting of missing lifetimes. The hope was that the compiller could simply output the reified function declarations, but it turns out that all that information is encoded in a completely different LLVM format and no easily translatable back to AST. (In general, after dropping levels, it's impossible to raise it up again, this is obviously useful in terms of making sure that the compiler is well structured, logical but from a tool perspective, being able to reverse these steps could prove very useful. In particular, this would help solve the problem of macros and the differing levels again). The idea in general is to count the lifetimes in the various positions (in/out, noting where self is) and then push vector describing partitions of the lifetime parameters. Then the pretty printer is used... The idea is to do a walk down the function declaration for lifetimes... 

\section{Known limitations}\label{S:limit}
\subsection{Forcing super-block conflicts}
With the current setup, there are cases where a super-block conflict which shadows nothing, or has no usages, may still be counted as a conflict. There are a few advantages of preventing redeclaration under the same name, one of which is that instead of checking every usage of every name, you only need to check the usages for the name that has been renamed. In the full compiler scenario, this is particularly bad and slow. The reason such checking is unnecessary is due to the fact that outsider usages suddenly binding to your new declaration is impossible -- the declaration is never shadowing anything.

\subsection{Macros}
Macros do not appear at all in the csv save-analysis output. Due to the incomplete information supplied by save-analysis, it should probably be encouraged that conflicts should be raised as a precautionary measure whenever possible. This makes it so that forcing super-block conflicts unnecessarily might actually help prevent conflicts with unseen macros. An additional run could identify issues with macros, however referring back to Opdyke's 7th constraint, the main issue is with sub-block or super-block conflicts which cannot be detected with any usual means and do not cause compilation errors. Pretending to handle macros is likely worse than not promising anything at all and an extra compilation run is bound to occur eventually. The following example highlights the behaviour shift with macros which you can see with println! as a macro:


\begin{verbatim}
simple.rs file:
fn main() {
    let a = 1;
    println!("{}", a);
}

% refactor var dxr-temp/unknown_crate.csv simple.rs  a:-1:-1 c
a -1 -1
NODE ID: 9
fn main() {
    let c = 1;
    println!("{}", a);
}
\end{verbatim}

%\subsection{Unsuccessful attempts at refactorings?}
%\subsubsection{Lessons learnt}